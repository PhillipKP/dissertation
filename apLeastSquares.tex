\chapter{Derivation of the Least Squares Estimator}\label{app:Derivation of the Least Squares Estimator}


Suppose 

\begin{equation}\label{eq:lineareqAp1}
	\mb{g} = \mb{Hr} 
\end{equation}

Given \smb{g} and \smb{H} we want to solve for \smb{r}. If the matrix is full rank then we can simply multiply both sides of equation \ref{eq:lineareqAp1} by $\mbi{H}$ 

\begin{equation}
	\mbi{H} \mb{g} = \mbi{H} \mb{Hr} = \mb{I} \mb{r} = \mb{r}
\end{equation}

If \smb{H} is not full rank then its inverse does not exist. However we can try to find a solution \smbh{r} that minimizes the least squared error. This is called the \emph{Least Squares Solution} also known as the \emph{Least Squares Estimator}, \emph{Ordinary Least Squares} and by many other names. We define the squared error as

\begin{equation}\label{eq:squarederrorAp1}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =    \mb{ \lVert \mb{Hr-g} \rVert }^2
\end{equation}

\noindent To minimize the error, we take the derivative of equation \ref{eq:squarederrorAp1} with respect to \smb{r} and set it equal to zero and solve for \smb{r}. Note that the equation \ref{eq:squarederrorAp1} can be expanded in terms to a inner product 

\begin{equation} \label{eq:squarederrorAp2}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =    \mb{ \lVert \mb{Hr-g} \rVert }^2 = \sum_{i=1}^{N} e_i^{2} = \mathbf{e}^T \mathbf{e} =(\mathbf{Hr-g} ) ^{T} ( \mathbf{Hr-g} )
\end{equation}

\noindent The transpose is distributive 

\begin{equation}
	 ( \mathbf{Hr-g} )^{T} = ( \mathbf{Hr} )^{T} - \mathbf{g}^{T} 
\end{equation}

\noindent The transpose of a product of matrices equals the product of their transposes in reverse order

\begin{equation}
	 ( \mathbf{Hr} )^{T} = \mathbf{r}^{T} \mathbf{H}^{T} 
\end{equation}

\noindent So equation \ref{eq:squarederrorAp2} becomes
\begin{equation}
	\begin{split}
		\mathbf{ \lVert \mathbf{e} \rVert }^2 & = ( \mathbf{r}^{T} \mathbf{H}^{T}  - \mathbf{g}^{T} )  ( \mathbf{Hr-g} ) \\
		& = \mathbf{r}^T \mathbf{H}^T \mathbf{H} \mathbf{r} - \mathbf{r}^T \mathbf{H}^T \mathbf{g} - \mathbf{g}^T \mathbf{H} \mathbf{r} + \mathbf{g}^T \mathbf{g}
	\end{split}
\end{equation}

\noindent We can see that the two middle terms $\mbt{r}\mbt{H}\mb{r} = \mbt{g}\mb{Hr}$ because they are just scalars.

\begin{equation}\label{eq:lsgradstep1}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r} - 2 \mathbf{g}^{T} \mathbf{H} \mathbf{r} +   \mathbf{g}^T \mathbf{g}  
\end{equation}	

To find the least squares solution, take the gradient with respect to \smb{r} and set it equal to zero. 

It should be noted that there are two different notations for writing the derivative of a vector with respect to a vector $\frac{ \partial{ \mb{y} } } { \partial{ \mb{x} } }$. If the numerator \smb{y} is of size $m$ and the denominator \smb{x} of size $n$, then the result can be laid out as either an $m \times n$ matrix or $n \times m$ matrix, i.e. the elements of \smb{y} laid out in columns and the elements of \smb{x} laid out in rows, or vice versa. They are both correct and equal, which leads to confusion when switching back in forth. I will write both to reduce confusion.

Clearly the gradient of the third term in equation \ref{eq:lsgradstep1} w.r.t $\mathbf{r}$ is $0$, so it goes away. We first tackle the first term on the right hand side in equation \ref{eq:lsgradstep1}

\begin{equation}
	\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r} 
\end{equation}

Let $\mathbf{K} = \mathbf{H}^T \mathbf{H}$. Since \smb{K} is symmetric, we can use the identity

\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{K} \mathbf{r} = 2 \mathbf{r}^T \mathbf{K} = 2 \mbt{K} \mb{r}
\end{equation}

since $\mathbf{K}=\mathbf{K}^T$ then 

\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r}  = 2 \mathbf{r}^T \mathbf{H}^T \mathbf{H} = 2 \mathbf{H}^T \mathbf{H} \mathbf{r}
\end{equation}



and the gradient of the middle term in equation \ref{eq:lsgradstep1} is simply $- 2 \mathbf{H}^{T} \mathbf{g}$ so 
\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} } \mathbf{ \lVert \mathbf{e} \rVert }^2  = 2 \mathbf{H}^T \mathbf{H} \mathbf{r} - 2 \mathbf{g}^{T} \mathbf{H}
\end{equation}
setting it equal to zero and solving for \smb{r} gives the least squares estimate
\begin{equation}
	\hat{ \mb{r} } = ( \mbt{H} \mb{H} )^{-1} \mbt{H} \mb{g}
\end{equation}