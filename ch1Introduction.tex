

\titleformat{\chapter}[display]
  {\normalfont\normalfont\huge\bfseries\color{black}}
  {\chaptertitlename\ \thechapter}{20pt}{\huge}

\chapter{Introduction}\label{chap:Introduction}

This chapter provides the motivation for the need to address the practical issues in experimental computational sensing. While computational sensing has the ability to ameliorate or eliminate trade-offs in many traditional isomorphic sensors, the sensor engineer is faced with a new of set challenges when designing a computational sensor. For example, while both computational and traditional isomorphic sensors often require calibration, a computational sensor can be more sensitive to calibration error due to the lack of redundancy in the measurement data. It is the author's hope that a full discussion on these various challenges will encourage more research on these issues. 

\Gls{isomorphic sensing} is the concept that the measurement data of a sensor resembles the signal-of-interest \cite{brady2009optical}. For example, in a camera, the digital image looks like the object. In isomorphic sensing the analog hardware, \acrfull{adc}, and processing algorithms are separate components, see Figure \ref{fig:isomorphicsesingflowchart}.  

\Gls{computational sensing} is the concept that a joint design of the sensor, often though active \gls{coding} (often called structured illumination) or passive \emph{coding} of the analog signal, with inversion algorithms will outperform the isomorphic sensor, see Figure \ref{fig:compSen} \cite{neifeld2006taskSpecificSensing}. An example of computational sensing is the \acrfull{afss}, where psuedo-arbitrary spectral filters adapt between measurements to quickly classify the input spectrum \cite{dinakarababu2011adaptive}. While \gls{isomorphic} sensors can provide flexible sensing in multiple applications, joint design of a \gls{computational sensor} will often lead to performance increases in resource constrained scenarios. Throughout this chapter and the rest of this dissertation, I will provide many examples that highlight the differences between computational and isomorphic sensing. 



\begin{figure}
    \centering
    \includegraphics[scale=1]{isomorphicsensorflowchart}
    \captionof{figure}[A systems view of a traditional sensing scheme.]{ The signal-of-interest is incident upon the analog instrument. The analog instrument forms an isomorphism of the signal which is then periodically sampled point-by-point through an \gls{adc} device. Once the signal is in digital form, post-processing algorithms are often used to perform various tasks such as noise reduction, detection, and classification. Notice that the analog instrument, sampling scheme, and processing are all seperated. }
    \label{fig:isomorphicsesingflowchart}
\end{figure}


\begin{figure}
	\includegraphics[scale=1]{compSen}
	\captionof{figure}[A systems view of a computational sensing scheme.]{In a computational sensor, coding, decoding, and structured illumination are used in addition to the analog instrument, the algorithms, and sampling scheme. The dashed boxes represent optional processes that may or may not be needed. }
	\label{fig:compSen}
\end{figure}

Rather than a rigorous discussion, this chapter will discuss some of the major developments and concepts in the field of computational sensing on an intuitive level. This will familiarize the reader with important terminology and techniques common in the field. This chapter will also discuss some of the challenges I and many other experimentalists and engineers have faced when developing computational sensing prototypes. I will close this chapter with a brief look ahead to the rest of the dissertation. 

%History%%%%%%%%%%%%%%%%%%%%

\section{Isomorphic Sensing}\label{sec:Isomorphic Sensing}

In Greek, the word isomorphic loosely translates to ``equal in form.'' Traditional sensors perform isomorphic sensing. In the context of this dissertation, an isomorphic sensor is any sensor which attempts to produces measurement data that resembles the signal-of-interest. In this paradigm, the analog instrument, sampling scheme, and post-processing algorithms are separate components and processes.

I will discuss three important examples of isomorphic sensors: the pinhole camera, the photographic camera and the optical spectrometer\footnote{I will call the optical spectrometer just a spectrometer from now on, even though there are many instruments called spectrometers that not concerned with optical spectra}. These sensors have inspired many other optical and non-optical sensors throughout history, so it is natural to use them as examples for comparison when discussing computational sensing. 

Before I continue, I want to define \gls{measurement} because it can often be used in an informal manner. In this dissertation a \gls{measurement} has a very specific meaning. A \gls{measurement} is a process that converts a physical phenomena to a collection of data. The signal-of-interest is the physical phenomena that one is interested in quantifying. I will call the collection of data the measurement data.  The word sensing has a less precise meaning and I will use it in an informal manner. Sensing is any act that uses techniques, instruments, or processes that produces measurement data. 

In the photographic camera, the signal-of-interest is the intensity distribution of the object. The analog instrument consists of the lenses which are designed and fabricated to produce an image that looks like the object at the \gls{fpa}. The better the optics the more the image resembles the object. The \gls{fpa} then samples the image and produces a digital representation of the intensity distribution of the object, the measurement data. If one is interested in performing a task such as detection or classification, the measurement data is sent to a post processing algorithm to perform those tasks. 

There are two major sub-systems in the photographic camera which determine how well it performs: the optics and the FPA. Ideally, the optics (the analog instrument in this case) will produce a \gls{psf} which is at the physical limit set by diffraction given the aperture size. For example, in a task such as the detection of a star from several neighboring stars in the night sky, if the \gls{psf} is much larger than the center to center seperation of the two stars in the optical image, it will be difficult to detect. Even if the \gls{psf} is small enough, the \gls{fpa} must sample at a fine enough pixel-to-pixel spacing, called the \gls{pixel pitch}, to accurately reproduce the intensity variations at the scale which is pertinent to the task. Intuitively, this makes sense because if the stars are imaged onto a single pixel, then one cannot ever hope to be able to accurately the detect the star without some other prior or side information. 

The pinhole camera consists of a small hole and a box which prevents any light except from the pinhole from entering, see Figure \ref{fig:pinholecamera}. The pinhole camera is useful for imaging in parts of the electromagnetic spectrum and particles for which there is no direct analog to the refractive lens or reflective mirror. Like the photographic camera, the smaller the \gls{psf} diameter, the better the spatial resolution. Unfortunately, in the traditional pinhole camera, the only way to reduce the \gls{psf} diameter is by decreasing the diameter of the pinhole which reduces the amount of light.

\begin{figure}
    \includegraphics[scale=1]{pinholecamera}
    \captionof{figure}[A pinhole camera.]{A pinhole camera is a simple imaging system that forms an image without a lens or mirror. This is due to the ray nature of light. A small hole will only admit a small amount of rays from an object point that is radiating light. Each point on a object emits light at different angles, and the image formed is a superposition of different rays. The smaller the hole, the less blurry the image. However, small holes also limit the amount of light}
    \label{fig:pinholecamera}
\end{figure}

In the spectrometer, the signal of interest is the spectrum of the object. The optics are designed to take the incoming light and separate various wavelength components, see Figure \ref{fig:slitspectrometer}. The part of the spectrometer which is used to physically isolate the wavelengths is called a \gls{monochromator}. The monochromator contains a prism or diffraction grating which creates a wavelength dependent angular seperation. The result is a spectral intensity as a function of position at the \gls{fpa}. The \gls{fpa} and post-processing algorithms are used in the same manner as the photographic camera, which is to sample the spatially varying intensity (which is now encoding spectral information) creating a digital version of it and to perform various tasks on the measurement data. For now, I will concentrate on the slit spectrometer, which measures the spectrum at a single spatial location on the object.


\begin{figure}
    \includegraphics[scale=1]{slitspectrometer}
    \captionof{figure}[An isomorphic slit spectrometer with a 4F configuration.]{An isomorphic slit spectrometer with a 4F configuration. The slit limits the lateral extent of the object (or intermediate image). The light from the slit is collimated and separated into different angles based on the wavelength. A second lens then images wavelength shifted copies of the slit on the image plane where the detector is. As the slit size decreases, less light is allowed, but one gains spectral resolution by rejected light from neighboring locations on the object. }
    \label{fig:slitspectrometer}
\end{figure}

In the spectrometer, one of the important performance metrics is \gls{spectral resolution}, which I denote \gls{specres}. The spectral resolution is the smallest difference in wavelength the instrument can discern. Large spectral resolutions can degrade the spectrometers ability to discern important parts of the spectrum. Similarly with the camera, the \gls{fpa} must have a \gls{pixel pitch} which is small enough to correctly sample the variations in the spectrum. 

The point-by-point nature of isomorphic sensing is both a strength and a source of weakness. The strength comes from the straightforward and intuitive architecture of the isomorphic sensor. Each subsystem: the optics, the \acrfull{fpa}, and the post-processing can be designed and constructed separately as long as they meet their individual specifications. As long as the \gls{snr} is sufficient and the sampling rate is high enough, one is guaranteed to recover the signal. 

One of the weaknesses of the isomorphic approach is the ability to measure low \gls{snr} signals. Because the signal-of-interest is sampled in a completely point-by-point at each exposure, each pixel indepdently contributes a certain amount of noise. If the noise dominates, the measurement fidelity decreases, forcing the operator to increase the exposure time. For weak signals, the exposure time can become prohibitive and for temporally dynamic signals this leads to a loss of temporal resolution. Indeed, one of the major engineering trade-offs faced by traditional spectrometer designers is that when one attempts to increase the light collection (increased slit-width) the spectral resolution \gls{specres} degrades. Similarly, in the pinhole camera, there is a throughput versus spatial resolution trade-off, increasing the size of the pinhole degrades the \gls{psf}.

It would be easy to assume that recent progress in machine learning and statistical signal processing combined with the dramatic increase in computing power that one could simply post-process poor measurements and obtain useful data. However, this is not possible due to the an important theorem in information theory called the \gls{data processing inequality} \cite{cover2012elements}. The information content of a signal cannot be increased through post-processing.


Another weakness of isomorhic sensing is that the seperation of the analog instrument, the sampling scheme, and the data processing algorithms lead to increased \gls{swap-c}. As I mentioned in the photographic camera, the optics must be designed to produce a small \gls{psf}. For demanding applications, the optical design and fabrication can be the most expensive component of the sensor. While the price of \gls{fpa}s sensitive to the visible wavelength region have fallen, \glspl{fpa} sensitive to certain parts of the electromagnetic spectrum can be quite expensive or non-existant \cite{watts2014terahertz, noor2011compressive}.

In many cases, the signal is redundant and high resolution sampling becomes a waste of resources, such as data storage and communications bandwidth. A good example is in photography where often the post-processing takes the digital image and applies a compression algorithm which looks for patterns in the signal and reduces the file size, discarding much of the sample data \cite{taubman2012jpeg2000}. 

The isomorphic sensor approach has served humanity well, however with all the weakness that I have discussed, there is a need for sensors which can operate in low-\gls{snr} conditions, with fewer measurement time, fewer measurements, or at lower \gls{swap-c} while still producing useful information relavent to the sensor task. I will now begin to discuss some of major techniques in computational sensing that can be used to address some or all of the issues that I just stated. 

\section{Development of Multiplexing in Sensing}\label{sec:multiplexInSensing}

\Gls{multiplexing} in sensing is the idea that each measurement sample is a physical combination of various parts of the analog signal-of-interest.  Multiplexing is a powerful tool that can be exploited by the sensor designer to eliminate or relax \gls{snr} related trade-offs. 

A simple example which illustrates the usefulness of multiplex sensing is weighing objects. In this example, one needs to weigh 100 sheets of paper. Assume that the measurement error of the scale is insignificant. Isomorphic sensing means one would need to measure each sheet of paper individually, requiring 100 measurements. 

If the measurement error of the scale is on the order of the weight of a single sheet, measuring each sheet individually produces a large measurement error. In order to reduce the error to an acceptable \gls{snr} one needs to make several measurements per sheet.

However, one can measure all 100 sheets at the same time. Since the weight of all 100 sheets is much larger than the measurement error of the scale, one can dramatically increase the precision of the measurement. If each sheet is the same weight, then the measurement process is finished.

The weighing problem is analogous to the spectroscopy example. As discussed earlier in \autoref{sec:Isomorphic Sensing}, there is trade-off between light collection and spectral resolution. Increasing the slit-width to increase the amount of light has the effect of degrading the spectral resolution \gls{specres}. Around the late 1940's and early 1950's, several important papers and inventions demonstrated the effectiveness of multiplexing in spectroscopy. At the time the \gls{fpa} was non-existant, so in the slit spectrometer shown in Figure \ref{fig:slitspectrometer}, where the \gls{fpa} is pictured, there was actually another slit. To record the intensity at each spectral channel, either the dispersive element or the exit slit had to be mechanically translated, making the measurements even slower by a factor of \gls{numspecchan}, the number of spectral channels of interest. 

Golay was the first to propose multiplexing the slit spectrometer by creating a pattern of binary (1's and 0's) entrance and exit slits \cite{golay1949multi}. In the Golay multi-slit spectrometer, the patterns of entrance and exit slits are matched based on mathematically useful properties. Intuitively, the ability to use multiple entrance and exit slits increases the optical throughput of the spectrometer. In communications theory, the process of structuring the data from the source to the receiver is referred to as \gls{coding}. Similarly, in computational sensing, the transmission of information between an object signal-of-interest and the sensor is considered a coding problem \cite{brady2009optical}. In the multi-slit spectrometer, the entrance slits act to code the spectrum while the exit slits decode the coded spectrum. Golay's idea dramatically increased the optical throughput without degrading the spectral resolution. 

Another example that is pertinent to this dissertation is coded aperture imaging. Coded aperture imaging can be thought of as the multiplexed version of a pinhole camera. As mentioned earlier in \autoref{sec:Isomorphic Sensing}, there is a trade-off between the throughput and spatial resolution. However in many fields, such as high-energy particle imaging, refractive lenses and reflective mirrors are non-existant or underdeveloped. By using multiple pinholes the throughput is increased without sacrificing spatial resolution. However, the pattern of the pinholes (which is the code) must be carefully designed in order for the reconstruction to be feasible. Fenimore, Canon, and Gottesman were among the first to create an elegant solution to coded aperture design called uniformly redundant arrays \cite{fenimore1978coded, gottesman1989new}. The uniformly redundant array increases throughput without significantly degrading the spatial resolution. 

In summary, multiplexing has the ability to eliminate classic trade-offs in isomorphic sensors: signal strength or resolution. Modern researchers are still actively developing novel ways to implement multiplexing to increase resolution and sensitivity in the spatial domain \cite{duarte2008single, townsend2012static}, spectral domain \cite{gehm2006static, tsai2013coded}, and temporal domain \cite{holloway2012flutter,llull2013coded}. However, multiplexing is not without its own set of challenges. As I mentioned, the coding must often be designed to obtain feasible signal reconstruction. I now discuss inverse problems in computational sensing.

\section{Forward Models and Inverse Problems}

In the computational sensing community, a model that explains the mapping of the signal-of-interest to the measurement data is called the \gls{forward model}. The problem of taking the observed data and calculating a reconstruction of the signal-of-interst or task-specific parameters is called the \gls{inverse problem}.

As you can imagine, solving inverse problems of isomorphic measurements, when one is concerned with reconstruction of the signal-of-interest, tend to be straightforward. In the weighing problem, the measurement is also the reconstruction. In the slit spectrometer, where the forward model can be simply the continuous to discrete mapping of the spectrum. The spectrum is the interpolated measurement. 

Of course, one can begin to add levels of complexity to the forward model to account for various physical aspects of the sensor, such as the fact the \gls{fpa} cannot measure certain wavelength regions or the noise in our measurements. But again, assuming proper sampling and enough \gls{snr}, the reconstruction of the isomorphic signal is the measurement. This simplicity is one reason why isomorphic sensing still dominates at the consumer level despite all of the drawbacks I discussed earlier in \autoref{sec:Isomorphic Sensing}. 

However, the multiplexing of signal information forces one to develop computational steps to solving the inverse problem. In the multiplexed weighing problem, a significant complication occurs when each sheet of paper has a different weight. Now solving the inverse problem is not as straightforward. 
In algebra, given only $1$ equation and $100$ unknowns, the problem is underdetermined. Similarly given a $1$ measurement of all 100 sheets is also an underdetermined problem. What one can do is try measuring different combinations of the 100 sheets, each new combination provides a new equation to work with reducing the error. Naively, one might assume that randomly choosing 100 unique combinations and solve 100 equations using algebra. This works fine when there is no measurement error. However, in the presence of noise, in many applications including the weighing problem, random combinations are not the best way to conduct the coding. They are sub-optimal in terms of reconstruction error. This lead many to begin working on optimal coding strategies of signals for sensing and is major topic in this disseration.

In summary, the forward model of a sensor is essentially accounting for the physics which govern the measurement. While the solving the inverse problem is a mathematical problem which attempts to either reconstruct the object or to calculate task-specific data from the measurement data. Unfortunately, not all multiplexing forward models codes have mathematically elegant inversion steps. Often the physics of the situation force non-isomorphic measurements which require a computational step to solve the inverse problem. 

\section{Indirect Imaging}

While Golay, Fennimore, and others were leveraging multiplexing to eliminate trade-offs in traditional sensors, an entirely disparate group of researchers were working on imaging techniques for which there was no isomorphic analog. In these cases the physics of the sensing modality prevents a point-by-point sampling of the signal-of-interest. Indirect imaging refers to sensing schemes which include X-ray \gls{ct}, \gls{spect}, \gls{pet}, \gls{mri} and certain forms of sonic and radio wave imaging. All require a data-processing or reconstruction step to solve an inverse problem \cite{barrett2013foundations}. 

Perhaps one of the most successful early examples of indirect imaging which led to the rise of inverse problems in sensing is the development of radar. While early radar was concerned with the detection and distance of an object, development of imaging radar began after World War II. Imaging radar and specifically \gls{sar} can use time delay information combined with the Doppler effect and interference of coherent radio waves to create high resolution images of terrain and buildings. 

In medicine, a common imaging modality is X-Ray \gls{ct}. In X-Ray \gls{ct}, computational inversion is required to reconstruct a 2 or 3-dimensional function from 1 or 2-dimensional measurement data. The forward model can be simple: In a collimated beam architecture with a 1-dimensional detector array, each sample from each pixel on the array is proportional to the total number of x-ray photons that have not been absorbed by the object \cite{radon20051}. The inversion relies on computing the inverse Radon transform \cite{nobelprize1979medicine}. 

Indirect imaging is a subfield of computational sensing. Due to the medical and military applications of these computational sensors, there has been an intense push to reduce measurement time and improve task-specific and reconstruction results. Many of the techniques from other subfields of computational sensing have been brought to bear for indirect imaging \cite{zhu2010tomographic, chen2012compressive}. 

I have discussed the development of multiplex sensing and indirect imaging and how the ideas from both subfields are analogous in terms of producing a non-isomorphic measurement. However a major step in practical implementation of computational sensing is being able to obtain the measurements in a quick, reliable and efficient manner. Computational sensing as a field would not exist without the most important invention in optics and photonics of the 20th century, the digital image sensor\footnote{ staring array, staring-plane array, focal-plane array (FPA), focal-plane, and image sensor are all synonymous }.

\section{The Digital Imaging Revolution}

The invention of the \gls{ccd} \gls{fpa} by Boyle and Smith in 1969 was a major breakthrough for entire fields and industries who depended on the reliable sampling, storage and transmission of optical signals \cite{boyle1970charge}. The \gls{ccd} is the first integrated circuit device that could reliably convert an optical image to a digital signal. Until then one either had to use film or bulky tubes that required an electron beam to be scanned across an image scene, such as the Image orthicon \cite{w1975image}. 

The invention of the digital camera by Sasson followed shortly after \cite{kodaksfirstdigital2015}. Several years later the first digital spectrometer was invented. In the digital spectrometer the exit slit of the monochromator was replaced by the \gls{ccd}, which allowed for instant and simultaneous measurement of the entire spectrum in a compact architecture \cite{moore1979spectrometer}. 

The development of the \gls{cmos} \gls{fpa} was also important. While in scientific settings, it could not rival the quality of the \gls{ccd}, its cheaper cost brought digital imaging to the consumer level. Other technology like the digital computer and computer networking also provided major contributions to the democratization of imaging and optical sensing. While scientific grade optical instrumentation was and is still expensive, the researcher could at least capture, process, and share measurement data with significantly less effort. Without it, the field of computational sensing would not exist.

Algorithms for efficient and reliable storage and transmission of digital images became more important. Over time the pixel count continued to increase and the sheer volume of digital image and video data being generated and transmitted over networks began to outpace improvements in storage and transmission capacity. While many engineers developed new technology to combat the hardware limitations of storage and transmission. This also led to a renewed effort by researchers to develop more efficient image and video compression algorithms \cite{kobayashi1974image, ziv1978compression}. 

Compression techniques all follow the same basic process, see Figure \ref{fig:imagecompressionflowchart}. Once the \gls{ccd} samples the optical signal and produces the measurement data, the encoder uses the compression algorithm to look for redundancies in the data and produce a compact representation of the signal-of-interest. The compressed data can then either be stored or transmitted or both. The decoder solves the inverse problem of reconstructing the image. 

\begin{figure}
    \centering
    \includegraphics[scale=1]{imagecompressionflowchart}
    \captionof{figure}[A general flowchart for image and data compression techniques.]{A general flowchart for image and data compression techniques. The digital image is analyzed by the encoder and compressed into a compressed form where it can be efficiently stored or transmitted. The decoder takes the compressed image and converts it back into an image that resembles the original digital image. }
    \label{fig:imagecompressionflowchart}
\end{figure}

In the next section I will discuss how many of the techniques and algorithms used in computational sensing are inspired by the techniques used by the image processing and digital communications community. This is because one of the major efforts of computational sensing is to make resource efficient measurements to reduce the total amount of measurements, whereas in image processing and communications, measurement data is often corrupted, missing, or too large for efficient storage and transmission.

\section{Compressive Sensing}\label{sec:multiplexingtocompressivesensing}

Traditionally, in order to increase the resolution of a sensor, one had to increase the number of measurements. This means that the \gls{swap-c} must also increase. A camera with just a few megapixels \gls{fpa} costs less than one with hundreds of megapixels. The cost of designing the optics will also need to scale to provide enough optical resolution. In a perfect world, one could capture all the information one needs from just a few measurements.

With a discrete signal one needs at least as many measurements as there are signal elements to solve the inverse problem. If the number of measurements is fewer, then the inverse problem is underdetermined. Conventional signal processing dictates that accurate reconstruction of the signal-of-interest is highly improbable. Fortunately, a signal acquisition technique called \gls{compressive sensing} allows one to design sensors that solve these types of highly underdetermined inverse problems.

As discussed earlier, much of the data being generated by sensors are redundant. Images, spectra, video, and audio data of real-world signals tend to exhibit patterns or redundancies that can be exploited. This allows a compression algorithm to significantly reduce the amount of data needed to represent the signal. 

There is a class of compression algorithms called lossy \cite{usevitch2001tutorial}. In lossy compression, not only are redundancies exploited but data that is deemed insignificant to the signal quality is discarded. Only the most important part of the signal is kept as part of the compressed representation of the original signal. When the signal is uncompressed, the amount of data is less than the original measured data. The difference in quality is often unnoticeable to a human observer. In both lossless and lossy compression, the goal is to obtain a \gls{sparse} representation of the signal. A sparse representation means that the signal can be well approximated with only a few non-zero elements in a representation basis. A representation basis is a basis in which the signal-of-interest is sparse. For example, most natural images are sparse in the Fourier basis. The representation basis is typically not the native basis of the signal-of-interest, i.e. pixel number or spectral channel.

Researchers pointed out that traditional sensors tend to produce vast amounts of measurement data, but often the majority of data is redundant and discarded in the compression step \cite{candes2006robust, candes2006near, donoho2006compressed}. This led to the idea that sensors can be designed to directly measure the most relavent data in a signal, suggesting a measurement scheme that can measure a compressed form of the signal. This is the idea behind \gls{compressive sensing} sometimes known as \gls{compressive sampling}. If the measurements are compressive then it should be possible to significantly reduce the number of measurements to accurately reconstruct the signal. 

Note that there is a distinction between compressive sensing and the traditional approach of sensing and then compressing. In the traditional approach, compression algorithms operate as a post-processing step. Therefore, a traditional compression algorithm will have access to the entire signal to look for redundancies and convert it into a sparse representation. In compressive sensing, one attempts the compression directly and therefore do not have access to the entire uncompressed signal. The algorithms must assume that the signal has a sparse representation. 

The question of how to actually measure or code the analog signal to directly obtain compressed data is also important. Fortunately, random coding tends to work well in many instances when the signal has a sparse representation. However in many cases, designed codes can significantly outperform random coding. I will discuss other types of coding schemes that can be used to outperform random codes. 

The idea of compressive sensing seems to be similar to the concept of multiplex sensing. However, there is an important distinction to be made. In compressive sensing, the aim is to obtain the relavent information in as few measurements as possible. In multiplexing, the goal is to overcome limitations mainly due to lack of \gls{snr}. Many compressive sensing schemes also employ multiplexing.

One useful example of compressive sensing versus traditional sensing is the single pixel camera \cite{duarte2008single}. The single pixel camera is a multiplexing camera architecture that uses time sequential random measurements and recovers the image in significantly fewer measurements (equal to number of exposures $\times$ pixels) than the conventional camera, see Figure \ref{fig:singlepixelcamera}. Another example is the \gls{cassi} architecture \cite{wagadarikar2008single}, which can reconstruct a spectral data cube in significantly fewer \gls{fpa} exposures than a traditional spectral imaging architecture. 


\begin{figure}
    \includegraphics[scale=1]{singlepixelcamera}
    \captionof{figure}[A single pixel camera architecture.]{A single pixel camera architecture. The object scene (or intermediate image) is imaged onto a \acrfull{dmd}. The each micro-mirror of \gls{dmd} reflects light towards the photodiode or to another direction. This acts as a point-by-point multiplication of the discrete image with the DMD pattern. The condenser lens sums any light reflected by the \gls{dmd} and focuses it onto the photodiode. This can be described mathematically as a vector multiplication of the image with the \gls{dmd} pattern. }
    \label{fig:singlepixelcamera}
\end{figure}

Another important distinction is between reconstruction and task-specific sensing. Task-specific sensing tends to refer to measurement techniques that attempt to directly perform tasks such as detection, classification, and estimation without the intermediate step of reconstructing the high-dimensional signal. Compressive sensing is useful not just of overcoming resolution limitations in reconstruction but for task-specific sensing. For example, in facial recognition the goal is detection of an individual person. Reconstruction of the face image is simply an intermediate step, therefore, one can develop a compressive sensing scheme that is optimal for direct facial detection, skipping the step of image reconstruction \cite{pal2005face}. 

Computational sensors can overcome classic engineering trade-offs in sensor design. However, there is a unique set of challenges related to computational sensing.

\section{Practical Considerations in Computational Sensing}

So far I have discussed the development of computational sensing techniques and how they are used to ameliorate trade-offs in traditional sensor design. Computational sensing as a field is continuing to grow at a rapid pace. The number of journal publications related to computational sensing has steadily increased every year since 2008 \cite{stern2016hurdles}. There is now a major \gls{osa} meeting dedicated to computational sensing \cite{cosi2016meetingwebsite} and textbooks dedicated to its study and development \cite{brady2009optical, foucart2013mathematical}. While it is a powerful approach to radically new sensor architectures, textbooks and papers tend to focus on architecture concepts and positive results. Little attention is given towards the practical issues one faces when implementing computational sensors. For example, calibration is a major topic in this dissertation. In many talks and papers on computational sensing, calibration is barely mentioned or relegated to a minor paragraph. 

Calibration is the process of quantifying the response of a sensor in order to produce an accurate forward model. While many traditional sensors also require calibration, computational sensors tend to be more sensitive to calibration error. 

There are two main reasons for this. The first reason is simply due to the fact that non-isomorphic measurements require a computational step to solve an inverse problem. The algorithms rely on accurate knowledge of the forward model to separate measurement data due to the instrument and data due to the signal-of-interest. 

The second reason is due to the lack of redundancy of measurement data in compressive sensing. The redundancies that are typically deemed wasteful in traditional sensing, can also be used by post-processing algorithms to solve the inverse problem in a robust fashion to correct for missing or corrupted data \cite{gehm2013calibration}. In \gls{compressive sensing}, only a few numbers are used to represent many. If the few numbers are misinterrepted due to poor calibration, it can have a drastic impact on the performance of the estimation algorithm. I will illustrate in this dissertation the calibration challenges in several computational sensors. 

Calibration has become a major drawback in compressive sensing. A consumer cannot be expected to spend time calibrating every time the instrument is physically bumped, the air temperature or pressure changes. In high dimensional compressive sensors like hyperspectral imagers, the calibration time can last hours.

One issue that is encountered in multiplexed sensing is lack of dynamic range. As the amount of light is sensed by the detector increases, it becomes more difficult to discern differences between signals. For example, in a single sinusoidally amplitude modulated signal, there is a DC offset and then the modulation itself. The pertinant information is encoded in the modulation so being able to resolve the peak to peak difference is important. Now imagine another amplitude signal with a different frequency. Physically, the amount of electrons in each pixel well is increased and the total modulation will tend to ``average'' out. The situation gets worse as the number of signals increases per measurement. This is one of the potential issues faced by single-pixel compressive sensing architectures. The \gls{scout} architecture attempts to alleviate this by ``spreading'' the photons onto more pixels for a compressive measurement. 

Another hurdle in the implementation of practical computational sensing is the need for prior knowledge. For example, in \gls{compressive imaging} one must assume the signal is sparse in some basis. Fortunately most realistic objects can be treated as such with many commonly known bases, such as a type of wavelet basis. However, sometimes one needs to image something that is difficult to represent in any known basis. One needs to resort to generating training data.  Training requires one to generate many different examples of the signal. This becomes time and computationally expensive. Another example for the need of prior knowledge is the \gls{afssi-c}, a computational spectral classifier which requires knowledge of the standard deviation of the probability density function of the noise to perform spectral classification in the least number of measurements as possible.

Reducing the number of detector elements is often the goal in computational sensing. A notable example is the single pixel camera is an architecture. However, the single pixel camera requires several time sequential measurements. Each measurement displays a different \gls{dmd} pattern to create a randomly encoded measurements \cite{duarte2008single}. The drawback to this architecture is that one must point the camera at the object scene until enough measurements have been collected for proper reconstruction. A complication arises when this architecture is used to image temporally varying object scenes. One must display the \gls{dmd} patterns even faster and reduce the exposure time to keep up. As a result the \gls{snr} may begin to degrade. 

A possible way to mitigate this issue is to do all the encoding in parallel. However, a completely parallel approach would require a lens, a \gls{dmd} (or coded aperture), and a detector pixel for each measurement. Since each lens uses a different entrance pupil, this means that each detector pixel will have a different view of the object scene. This drastically scales the complexity of the architecture and algorithms. In this dissertation, I will discuss a compromise to parallel coding, in two different computational sensors, by using a common entrance pupil and a \gls{ccd}.

Much of the optimal measurement codes contain simultaneously positive and negative measurement weights. In reality, with incoherent light one is unable to make negative measurements. One is often forced into situations where one must record two sets of measurements and subtract one set of measurements from the positive set of measurements. This means an additional noise term is added to each effective measurement. 

Algorithms engineered to solve inverse problems often do not account for the non-negativity of many physical situations. For example, in spectral unmixing where the problem is to solve for the concentration of each material given a mixed spectrum. A non-negative fractional abundance that sums to one is a physical requirement. However, there is a lack of sparsity promotting algorithms that are able to enforce both the non-negativity and the sum to one constraint. 

A major issue in both the theoretical and experimental compressive sensing community is a lack of code design schemes. For the most part random measurements techniques are dominant because they obey well known theoretical results which guarantee reconstruction with high probability \cite{donoho2006compressed, candes2008introduction, tropp2006just}. However, designed codes have been shown to outperform random codes in various applications \cite{dinakarababu2011adaptive, pal2005face, dunlop2016experimental, ashok2011compressive, li2014realization}. Intuitively, designed codes which can take into account prior knowledge of the physical limitations of the sensing task and additional statistical assumptions of the signal-of-interest should be able to outperform random measurements. For example, I will demonstrate in the \gls{afssi-c} that adaptively designed \gls{pca} codes dramatically outperform random codes in low \gls{snr} environments. 

These issues and others will be discussed in a case study manner throughout this dissertation. I will now discuss the organization of this dissertation and the three separate computational sensors I have built and how I have tried to mitigate and resolve some of the practical issues associated with computational sensing.

\section{Dissertation Overview}

I used this chapter to cover the major concepts of computational sensing. More importantly, I provided the major topic of discussion for this dissertation, the challenges that remain towards developing practical computational sensors. I will use \Cref{chap:Formalism} as an opportunity to provide a more detailed and mathematically rigorous look at the various coding schemes that are popular in computational sensing, as well as the ones I have used. This includes a discussion of the mathematical methods that I and my collaborators developed and deployed in the \gls{afssi-c}: Bayes rule, Log-Likelihood Ratios, and the Maximum a Posteriori decision technique. A more rigorous treatment of compressive sensing will also be provided which includes several important results from the compressive sensing community. I will also discuss several estimation and task-specific sensing algorithms that I used during the development of the three computational sensors in my work.

\Cref{chap:Scout} will introduce a new system architecture for compressive target tracking---the Static Computational Optical Undersampled Tracker (SCOUT). This system is designed to overcome a variety of challenges that typically arise in traditional optical sensing approaches. It provides several advantages over the single-pixel camera architecture, notably it can capture many simultaneous encodings of the field-of-view with a single camera exposure. I will present experimental results that validate the performance of the proposed architecture. 

\Cref{chap:Afssic} will discuss an important experimental prototype which demonstrates adaptive spectral image classification. The system is a major experimental advancement compared to current computational spectral imaging architectures which focus on reconstruction. Furthermore, the ability to perform adaptive measurements, where each code is designed based on the history of prior measurement data allows this instrument to outperform traditional spectral imaging architectures by a factor of 100 in low \gls{snr} scenarios.

\Cref{chap:Csu} will discuss current efforts to experimentally demonstrate compressive spectral unmixing. Often in spectral imaging, the spectra present in the field-of-view of each pixel is actually a mixture of several spectra, a mixed spectrum. Spectral unmixing is the task of inferring the fraction of each constituent spectrum in the mixed spectrum. I employ a unique architecture for computational spectral sensing which uses no diffraction gratings or refracting prisms but relies on the wavelength dependent nature of the birefringence in an \gls{lcos}. I will demonstrate the effectiveness of various compressive sensing measurement schemes to outperform a traditional sensor in significantly fewer measurements. 

\Cref{chap:Conclusion} will summarize the dissertation and provide my perspective on how the field should the approach the issues in practical computational optical sensing.


%\bibliographystyle{IEEEtranS}  
%\bibliography{ThesisBib}

