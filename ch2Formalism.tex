\chapter{Formalism}\label{chap:Formalism}

This chapter introduces the reader to the more rigorous concepts and mathematical background that will required to fully understand the material presented in the later chapters of this dissertation. 

A rigorous discussion of multiplexing and signal-to-noise ratio will be discussed, as well are various coding schemes used in various notable computational sensors as well as the ones in this disseration. 

Since the \gls{afssi-c} relies on a variation of Principal Component Analysis (PCA) and a Bayesian algorithm for coding design we will discuss some of the fundamental of Bayesian probability and the Log-Likelihood Ratios. 


\section{Isomorphic Sensing}

\begin{equation}
\mathbf{g} = \mathbf{Hr}
\end{equation}

where \gls{measvec} is the measurement vector.

\section{Multiplexing}

\section{Principal Component Analysis}

\section{Bayesian Rules and Log-Likelihood Ratios}

\section{Compressive Sensing}

\subsection{The Nyquist-Shannon Sampling Theorem}

The Nyquist-Shannon Sampling Theorem states one must sample a signal with a sampling rate that is at least twice the maximum frequency of the signal to prevent aliasing \cite{shannon1949communication}.


\subsection{Sparsity, Incoherence, and the Restricted Isometry Property}

\subsection{Inversion}

\subsubsection{Least Squares}

Suppose $\mathbf{g} = \mathbf{H} \mathbf{r}$

We want to solve for $\mathbf{r}$ given $\mathbf{g}$ and $\mathbf{H}$. What we can do is define a residual

\begin{equation}
	\mathbf{e} = \mathbf{Hr-g}
\end{equation}
and attempt to find a proposed solution $\mathbf{r}$ that makes the residual square of $\mathbf{e}$ small as possible. This is called the Least Squares Solution also known as the Least Squares Estimator. 

Note that 

\begin{equation}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =  \sum_{i=1}^{N} e_i^{2} = \mathbf{e}^T \mathbf{e} =( \mathbf{Hr-g} ) ^{T} ( \mathbf{Hr-g} )
\end{equation}

The transpose is linear 

\begin{equation}
	 ( \mathbf{Hr-g} )^{T} = ( \mathbf{Hr} )^{T} - \mathbf{g}^{T} 
\end{equation}

The transpose of a product of matrices equals the product of their transposes in reverse order

\begin{equation}
	 ( \mathbf{Hr} )^{T} = \mathbf{r}^{T} \mathbf{H}^{T} 
\end{equation}

So 
\begin{equation}
	\begin{split}
		\mathbf{ \lVert \mathbf{e} \rVert }^2 & = ( \mathbf{r}^{T} \mathbf{H}^{T}  - \mathbf{g}^{T} )  ( \mathbf{Hr-g} ) \\
		& = \mathbf{r}^T \mathbf{H}^T \mathbf{H} \mathbf{r} - \mathbf{r}^T \mathbf{H}^T \mathbf{g} - \mathbf{g}^T \mathbf{H} \mathbf{t} + \mathbf{g}^T \mathbf{g}
	\end{split}
\end{equation}
let $\mathbf{k} = \mathbf{Hr}$ then we can see the two middle terms are just the inner product of two vectors which commutes, $\mathbf{g}^T \mathbf{k} = \mathbf{k}^T \mathbf{g}$
\begin{equation}\label{eq:lsgradstep1}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r} - 2 \mathbf{g}^{T} \mathbf{H} \mathbf{r} +   \mathbf{g}^T \mathbf{g}  
\end{equation}	
To find the least squares solution, take the gradient w.r.t $\mathbf{r}$ and set it equal to zero. The gradient of the constant term goes away. We first tackle the first term on the right hand side in equation \ref{eq:lsgradstep1}
\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r} 
\end{equation}
Since $\mathbf{K} = \mathbf{H}^T \mathbf{H}$ we can use the identity
\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{K} \mathbf{r} = \mathbf{r}^T ( \mathbf{K} + \mathbf{K}^T)
\end{equation}
since $\mathbf{K}=\mathbf{K}^T$ then 
\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} }  \mathbf{r}^{T} \mathbf{H}^{T} \mathbf{H} \mathbf{r}  = 2 \mathbf{r}^T \mathbf{H}^T \mathbf{H} = 2 \mathbf{H}^T \mathbf{H} \mathbf{r}
\end{equation}
The last step is realizing the middle statement is a row vector but we want a column vector so if we apply the transpose and again remember that $ \mathbf{H}^T \mathbf{H} $ is symmetric we get the statement on the right.   Clearly the gradient of the third term in equation \ref{eq:lsgradstep1} w.r.t $\mathbf{r}$ is $0$ and the gradient of the middle term in equation \ref{eq:lsgradstep1} is simply $- 2 \mathbf{H}^{T} \mathbf{g}$ so 
\begin{equation}
\dfrac {\partial } {\partial \mathbf{r} } \mathbf{ \lVert \mathbf{e} \rVert }^2  = 2 \mathbf{H}^T \mathbf{H} \mathbf{r} - 2 \mathbf{g}^{T} \mathbf{H}
\end{equation}
setting it equal to zero and solving for r gives the least squares estimate
\begin{equation}
	\hat{\mathbf{r}} = ( \mathbf{H}^{T} \mathbf{H} )^{-1} \mathbf{H}^T \mathbf{g}
\end{equation}

\subsubsection{L0 and L1 Norm Minimization}

\subsubsection{LASSO and sparsity regularization}

%\bibliographystyle{IEEEtranS}  
%\bibliography{ThesisBib}



