\chapter{Formalism}\label{chap:Formalism}

This chapter introduces the reader to the more rigorous concepts and mathematical background that will required to fully understand the material presented in the later chapters of this dissertation. 

A rigorous discussion of multiplexing and signal-to-noise ratio will be discussed, as well are various coding schemes used in various notable computational sensors as well as the ones in this disseration. 

Since the \gls{afssi-c} relies on a variation of Principal Component Analysis (PCA) and a Bayesian algorithm for coding design we will discuss some of the fundamental of Bayesian probability and the Log-Likelihood Ratios. 


\section{Isomorphic Sensing}

\begin{equation}
\mathbf{g} = \mathbf{Hr}
\end{equation}

where \gls{measvec} is the measurement vector.

\section{Multiplexing}

\section{Principal Component Analysis}

\section{Bayesian Rules and Log-Likelihood Ratios}

\section{Compressive Sensing}

\subsection{The Nyquist-Shannon Sampling Theorem}

The Nyquist-Shannon Sampling Theorem states one must sample a signal with a sampling rate that is at least twice the maximum frequency of the signal to prevent aliasing \cite{shannon1949communication}.


\subsection{Sparsity, Incoherence, and the Restricted Isometry Property}

\subsection{Inversion}

\subsubsection{Least Squares}

Suppose 

\begin{equation}\label{eq:lineareq1}
	\mb{g} = \mb{Hr} 
\end{equation}

Given \smb{g} and \smb{H} we want to solve for \smb{r}. If the matrix is full rank then we can simply multiply both sides of equation \ref{eq:lineareq1} by $\mbi{H}$ 

\begin{equation}
	\mbi{H} \mb{g} = \mbi{H} \mb{Hr} = \mb{I} \mb{r} = \mb{r}
\end{equation}

If \smb{H} is not full rank then its inverse does not exist. However we can try to find a solution \smbh{r} that minimizes the least squared error. This is called the \emph{Least Squares Solution} also known as the \emph{Least Squares Estimator}, \emph{Ordinary Least Squares} and by many other names. We define the squared error as

\begin{equation}\label{eq:squarederror1}
	\mathbf{ \lVert \mathbf{e} \rVert }^2 =    \mb{ \lVert \mb{Hr-g} \rVert }^2
\end{equation}

\noindent To minimize the error, we take the derivative of equation \ref{eq:squarederror1} with respect to \smb{r} and set it equal to zero and solve for \smb{r}. The full derivation which shows each step is given in Appendix \ref{app:Derivation of the Least Squares Estimator}. The least squares estimate:

\begin{equation}
	\hat{ \mb{r} } = ( \mbt{H} \mb{H} )^{-1} \mbt{H} \mb{g}
\end{equation}

\subsubsection{L0 and L1 Norm Minimization}

\subsubsection{LASSO and sparsity regularization}

%\bibliographystyle{IEEEtranS}  
%\bibliography{ThesisBib}



